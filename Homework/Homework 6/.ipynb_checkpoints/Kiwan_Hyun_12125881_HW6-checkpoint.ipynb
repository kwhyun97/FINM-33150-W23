{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<left>FINM 33150 - Quantitative Trading Strategies</left>\n",
    "<left>Winter 2023</left>\n",
    "<br>\n",
    "<h1><center> Homework 6: Predictive Regression </center></h1>\n",
    "<center>Due - 23:00 [CST] February 17, 2023</center>\n",
    "<br>\n",
    "<h3>Ki Hyun</h3>\n",
    "<h3>Student ID: 12125881</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Imports </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import quandl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from statsmodels.regression import linear_model as lm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Constants </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prices_table = 'QUOTEMEDIA/PRICES'\n",
    "CDS_table = os.path.join(r'C:\\Users\\kwhyu\\OneDrive - The University of Chicago\\2023-1 Winter\\FINM 33150\\FINM-33150-W23',\n",
    "                         r'Data\\CDS\\Liq5YCDS.delim')\n",
    "K = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Helper Functions </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code was given by Dr. Boonstra, B., Ph.D. for\n",
    "University of Chicago FINM 33150 Quandl Options Data Fetching guidelines\n",
    "\"\"\"\n",
    "def grab_quandl_table(\n",
    "    table_path,\n",
    "    avoid_download=False,\n",
    "    replace_existing=False,\n",
    "    date_override=None,\n",
    "    allow_old_file=False,\n",
    "    **kwargs,\n",
    "):\n",
    "    if os.environ['OS'][0:7] == \"Windows\":\n",
    "        root_data_dir = os.path.join(os.environ['HOMEPATH'], 'Quandl Data')\n",
    "    else:\n",
    "        root_data_dir = os.path.join(os.environ[\"HOME\"], 'Quandl Data')\n",
    "    data_symlink = os.path.join(root_data_dir, f\"{table_path}_latest.zip\")\n",
    "    if avoid_download and os.path.exists(data_symlink):\n",
    "        print(f\"Skipping any possible download of {table_path}\")\n",
    "        return data_symlink\n",
    "\n",
    "    table_dir = os.path.dirname(data_symlink)\n",
    "    if not os.path.isdir(table_dir):\n",
    "        print(f'Creating new data dir {table_dir}')\n",
    "        os.mkdir(table_dir)\n",
    "\n",
    "    if date_override is None:\n",
    "        my_date = datetime.now().strftime(\"%Y%m%d\")\n",
    "    else:\n",
    "        my_date = date_override\n",
    "    data_file = os.path.join(root_data_dir, f\"{table_path}_{my_date}.zip\")\n",
    "\n",
    "    if os.path.exists(data_file):\n",
    "        file_size = os.stat(data_file).st_size\n",
    "        if replace_existing or not file_size > 0:\n",
    "            print(f\"Removing old file {data_file} size {file_size}\")\n",
    "        else:\n",
    "            print(\n",
    "                f\"Data file {data_file} size {file_size} exists already, no need to download\"\n",
    "            )\n",
    "            return data_file\n",
    "\n",
    "    dl = quandl.export_table(\n",
    "        table_path, filename = data_file, api_key = 'JbMPn9bSpFPNS7Z7PcZy', **kwargs\n",
    "    )\n",
    "    file_size = os.stat(data_file).st_size\n",
    "    if os.path.exists(data_file) and file_size > 0:\n",
    "        print(f\"Download finished: {file_size} bytes\")\n",
    "        if not date_override:\n",
    "            try:\n",
    "                if os.path.exists(data_symlink):\n",
    "                    print(f\"Removing old symlink\")\n",
    "                    os.unlink(data_symlink)\n",
    "                print(f\"Creating symlink: {data_file} -> {data_symlink}\")\n",
    "                os.symlink(\n",
    "                    data_file, data_symlink,\n",
    "                )\n",
    "            except:\n",
    "                print(f\"Symlink Creation Permission Denied\")\n",
    "                data_symlink = data_file\n",
    "    else:\n",
    "        print(f\"Data file {data_file} failed download\")\n",
    "        return\n",
    "    return data_symlink if (date_override is None or allow_old_file) else \"NoFileAvailable\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This code was given by Dr. Boonstra, B., Ph.D. for\n",
    "University of Chicago FINM 33150 Quandl Options Data Fetching guidelines\n",
    "\"\"\"\n",
    "def fetch_quandl_table(table_path, avoid_download=True, **kwargs):\n",
    "    return pd.read_csv(\n",
    "        grab_quandl_table(table_path, avoid_download=avoid_download, **kwargs),\n",
    "        low_memory = False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file \\Users\\kwhyu\\Quandl Data\\QUOTEMEDIA/PRICES_20230217.zip size 1378907642 exists already, no need to download\n"
     ]
    }
   ],
   "source": [
    "Prices = fetch_quandl_table(Prices_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_CDS_data(CDS_df, returns = True, window = 1):\n",
    "    df = CDS_df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df[df['date'].apply(lambda x: x.weekday()) == 2] # Wednesday to Wednesday only\n",
    "    df.set_index('date', inplace = True)\n",
    "    df = df[['parspread','ticker']]\n",
    "    # calculating Index Data\n",
    "    CDS_index = df.index.unique().map(lambda x: df.loc[x]['parspread'].sum()).values\n",
    "    df_index = pd.DataFrame(CDS_index, index = df.index.unique(), columns = ['parspread'])\n",
    "    df_index['ticker'] = 'Index'\n",
    "    # adding Index Data to the main dataframe\n",
    "    df =  pd.concat([df, df_index])\n",
    "    # sorting by date and ticker\n",
    "    df.reset_index(inplace = True)\n",
    "    df.sort_values(['date', 'ticker'], inplace = True)\n",
    "    df.set_index(['date', 'ticker'], inplace = True)\n",
    "\n",
    "    if returns:\n",
    "        df['returns'] = df.groupby('ticker').pct_change(periods = window)\n",
    "        df = df[['returns']]\n",
    "\n",
    "    return df.stack().unstack(level = 1).reset_index(level = 1, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_Prices_data(tickers, start_date, end_date, returns = True, window = 1):\n",
    "    global Prices\n",
    "    # adding S&P 500 ETF Ticker\n",
    "    tickers.append('SPY')\n",
    "    # filtering tickers and date\n",
    "    df = Prices[(Prices['date'] >= (start_date - timedelta(days = 7)).strftime('%Y-%m-%d')) &\n",
    "                (Prices['date'] <= end_date.strftime('%Y-%m-%d')) &\n",
    "                (Prices['ticker'].isin(tickers))].copy()\n",
    "    # changing date to datetime object\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df[df['date'].apply(lambda x: x.weekday()) == 2] # Wednesday to Wednesday only\n",
    "    # setting the index to date and ticker\n",
    "    df.set_index(['date', 'ticker'], inplace = True)\n",
    "    df.sort_index(inplace = True)\n",
    "    df = df[['adj_close']]\n",
    "\n",
    "    if returns:\n",
    "        df = df.groupby('ticker').pct_change(periods = window)\n",
    "        df = df.rename(columns = {'adj_close': 'returns'})\n",
    "\n",
    "    return df.stack().unstack(level = 1).reset_index(level = 1, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sherman_Morrison_OLS:\n",
    "    def __init__(self, X, y):\n",
    "        # ensuring matrix operations\n",
    "        X = np.atleast_2d(X)\n",
    "        y = np.atleast_2d(y)\n",
    "        # initial regression\n",
    "        ## inverse matrix\n",
    "        self.P = inv(np.matmul(X.T, X))\n",
    "        ## coefficients\n",
    "        initial_beta = np.matmul(np.matmul(self.P, X.T), y)\n",
    "        self.beta = initial_beta\n",
    "        self.initial = initial_beta\n",
    "        self.resid = []\n",
    "\n",
    "    def update_P(self, x, f, dir = 1):\n",
    "        P_new = self.P - np.matmul(np.matmul(self.P, x), np.matmul(x.T, self.P))/f*dir\n",
    "        return P_new\n",
    "\n",
    "    def update_beta(self, x_new, y_new):\n",
    "        # error dispersion\n",
    "        f = 1 + np.matmul(np.matmul(x_new.T, self.P), x_new)[0,0]\n",
    "        # prediction error\n",
    "        h = y_new - np.matmul(x_new.T, self.beta)[0,0]\n",
    "        self.resid.append(h)\n",
    "        beta_new = self.beta + np.matmul(self.P, x_new) * h / f\n",
    "        self.P = self.update_P(x_new, f, dir = 1)\n",
    "        return beta_new\n",
    "\n",
    "    def reduce_beta(self, x_old, y_old):\n",
    "        # error dispersion\n",
    "        f = 1 - np.matmul(np.matmul(x_old.T, self.P), x_old)[0,0]\n",
    "        # prediction error\n",
    "        h = y_old - np.matmul(x_old.T, self.beta)[0,0]\n",
    "        beta_reduced = self.beta - np.matmul(self.P, x_old) * h / f\n",
    "        self.P = self.update_P(x_old, f, dir = -1)\n",
    "        return beta_reduced\n",
    "\n",
    "    def window_update(self, x_new, y_new, x_old, y_old):\n",
    "        # ensuring matrix operations\n",
    "        x_new = np.atleast_2d(x_new).T\n",
    "        x_old = np.atleast_2d(x_old).T\n",
    "        # update with new data\n",
    "        self.beta = self.update_beta(x_new, y_new)\n",
    "        # delete old data\n",
    "        self.beta = self.reduce_beta(x_old, y_old)\n",
    "        return self.beta\n",
    "\n",
    "    def exponential_decay(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CDS_boxcar(ticker, K, CDS, Equity):\n",
    "    # regression data\n",
    "    y = CDS[[ticker]]\n",
    "    X = Equity[[ticker, 'Index']]\n",
    "    # boxcar regression\n",
    "    temp = Sherman_Morrison_OLS(X.iloc[:K, :].values, y.iloc[:K, :].values)\n",
    "    ## updating betas\n",
    "    betas = np.array(list(map(lambda x: temp.window_update(X.iloc[K+x, :].values, y.iloc[K+x, :].values[0],\n",
    "                                                           X.iloc[0+x, :].values, y.iloc[0+x,:].values[0])[:,0],\n",
    "                              range(len(Equity.index) - K))))\n",
    "    # tag for dataframe\n",
    "    resid_tag = r'$\\rho_{' + ticker + r'}'\n",
    "    beta_1_tag = r'$\\beta_{' + ticker + r', Equity}'\n",
    "    beta_2_tag = r'$\\beta_{' + ticker + r', Index}'\n",
    "    # indexes\n",
    "    grid = y.iloc[K:, :].index\n",
    "    # pd Series\n",
    "    resid = pd.Series(temp.resid, index = grid, name = resid_tag)\n",
    "    beta_1 = pd.Series(betas[:, 0], index = grid, name = beta_1_tag)\n",
    "    beta_2 = pd.Series(betas[:, 1], index = grid, name = beta_2_tag)\n",
    "    return resid, beta_1, beta_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 1. Introduction </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 2. Data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDS_raw = pd.read_table(CDS_table, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDS = clean_CDS_data(CDS_raw)\n",
    "CDS.head(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Equity = clean_Prices_data(list(CDS_raw.ticker.unique()), CDS.index[0], CDS.index[-1])\n",
    "Equity.head(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_list_1 = CDS[CDS.index.isin(Equity.index) == False].index\n",
    "if len(missing_list_1) == 0:\n",
    "    print(\"There were no dates included in the CDS Data but not in the Prices Data\")\n",
    "else:\n",
    "    for date in missing_list_1:\n",
    "        print(\"Return on\", date.strftime('%Y-%m-%d'), \"was included in the CDS data but not in the Prices data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_list_2 = Equity[Equity.index.isin(CDS.index) == False].index\n",
    "if len(missing_list_2) == 0:\n",
    "    print(\"There were no dates included in the Prices Data but not in the CDS Data\")\n",
    "else:\n",
    "    for date in missing_list_2:\n",
    "        print(\"Return on\", date.strftime('%Y-%m-%d'), \"was included in the Prices data but not in the CDS data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_list_3 = Equity.index[1:][Equity.index.map(lambda x: x + timedelta(days = -7)\n",
    "                                                   ).values[1:] != Equity.index.values[:-1]]\n",
    "if len(missing_list_3) == 0:\n",
    "    print(\"There were no dates included in the Prices Data that calculated 2-week return\")\n",
    "else:\n",
    "    for date in missing_list_3:\n",
    "        print(\"Return on\", date.strftime('%Y-%m-%d'), \"was calculated as 2-week return in the Prices Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_list_4 = CDS.index[1:][CDS.index.map(lambda x: x + timedelta(days = -7)).values[1:] != CDS.index.values[:-1]]\n",
    "if len(missing_list_4) == 0:\n",
    "    print(\"There were no dates included in the CDS Data that calculated 2-week return\")\n",
    "else:\n",
    "    for date in missing_list_4:\n",
    "        print(\"Return on\", date.strftime('%Y-%m-%d'), \"was calculated as 2-week return in the CDS Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_grid = Equity.index[Equity.index.isin(missing_list_3) == False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CDS = CDS.loc[agg_grid]\n",
    "Equity = Equity.loc[agg_grid].merge(CDS[['Index']], how = 'inner', left_index = True, right_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 3. Methodology </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 3-0. Subheading </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = CDS_boxcar('BA', K, CDS, Equity)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 4. Analysis </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 4-0. Subheading </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> 5. Evaluation </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> 5-0. Subheading </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
